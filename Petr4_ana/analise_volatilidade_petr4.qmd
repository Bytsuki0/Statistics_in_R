---
title: "Análise de Volatilidade Estocástica e Retornos Intradiários - PETR4"
author: "Gustavo Vitor da Silva"
format: html
editor: visual
---

## Introdução

Este relatório explora o comportamento da volatilidade e dos retornos da ação PETR4 ao longo de 2021, com dados de frequência de 1 minuto extraídos da plataforma MetaTrader. Utilizamos modelos de volatilidade estocástica via `stochvol`, comparações com volatilidade realizada, e examinamos propriedades de clustering de volatilidade.

## Leitura e Pré-processamento dos Dados

-   Remoção de janelas de abertura com menor liquidez (10:00 até 10:20) e dos últimos minutos de pregão (após 16:55).

-   Exclusão do feriado em 17/02/2021.

-   Conversão para série temporal (`xts`) e remoção de outliers extremos substituindo-os pela observação anterior

```{r load-packages, include=FALSE}
  library(ggplot2)
  library(tidyr)
  library(zoo)
  library(xts)
  library(forecast)
  library(lubridate)
  library(PerformanceAnalytics)
  #library(highfrequency)         
  library(quantmod)    
  library(tseries)     
  library(FinTS)       
  library(stochvol)  
  library(fpp2)     
  library(fpp3)       
  #library(modeltime)    
  #library(timetk)      
  library(parsnip)      
  library(rsample)      
  library(cowplot)
  library(wavelets)
  library(gridExtra)
  library(broom)
  library(rugarch) 
  library(moments)  
  library(coda)
  library(ggplot2)
  library(car)       
  library(boot)
```

```{r}
os <- Sys.info()["sysname"]
if(os == "Windows") {
  dt.intra <- read.csv("D:/Code/R_studio/Petr4_ana/dt_1min_PETR4_2021_metatrader.csv", 
                       header = TRUE, stringsAsFactors = FALSE, 
                       sep = ";", dec = ",")
} else if(os == "Linux") {
  dt.intra <- read.csv("~/Documents/Coding/Statistics_in_R/Petr4_ana/dt_1min_PETR4_2021_metatrader.csv", 
                       header = TRUE, stringsAsFactors = FALSE, 
                       sep = ";", dec = ",")
}

dt1 <- as_tibble(dt.intra) %>% 
  mutate(Period = ymd_hms(X)) %>%
  select(-X) %>% 
  filter(!(hour(Period) == 10 & minute(Period) < 20)) %>% 
  filter(hour(Period) < 17) %>% 
  filter(!(hour(Period) == 16 & minute(Period) > 54)) %>% 
  filter(!(date(Period) == "2021-02-17")) %>% 
  arrange(Period)

ret.1min <- as.xts(dt1$Ret.1min, order.by = dt1$Period)

```

## Observação inicial

```{r}
plot(ret.1min, main="PETR4 Retornos 1min", col="black")
boxplot(as.double(ret.1min), main="Boxplot dos Retornos 1min")
```

## Testes de Estacionariedade

```{r}
tseries::adf.test(ret.1min)
```

Resultado mostra que os retornos são estacionários, o que é esperado para séries de retornos.

## Analises de têndencias pré e pós queda

```{r}
  idx_min <- which.min(dt1$Ret.1min)

  dados_xts <- xts(dt1[, c("Close.1min", "Ret.1min")], order.by = dt1$Period)

  p2 = idx_min +10000
```

```{r}
dados_xts$LogRet.1min <- log(1 + dados_xts$Ret.1min)

dados_xts <- na.omit(dados_xts)

calc_volatilidade_historica <- function(retornos, janela = 21) {
  vol_hist <- rollapply(retornos, width = janela, 
                        FUN = function(x) sd(x, na.rm = TRUE), 
                        by.column = TRUE, 
                        align = "right")
  return(vol_hist)
}

dados_xts$Vol_Hist_30min <- calc_volatilidade_historica(dados_xts$LogRet.1min, janela = 30)

minutos_ano <- 252 * 390
dados_xts$Vol_Anual <- dados_xts$Vol_Hist_30min * sqrt(minutos_ano)

volhist30min=dados_xts$Vol_Hist_30min

plot(volhist30min[1:10000], main = "Volatilidade (Janela 30 min) Pré queda", xlab = "Data", ylab = "Volatilidade")

plot(volhist30min[idx_min:p2], main = "Volatilidade (Janela 30 min) Pós queda", xlab = "Data", ylab = "Volatilidade")
```

```{r}
warning=FALSE
message=FALSE
log_ret_vec <- as.numeric(dados_xts$LogRet.1min)
ret_vec <- as.numeric(dados_xts$Ret.1min)


# Visualizando ACF e PACF
par(mfrow = c(1, 2))
acf(log_ret_vec[1:10000], main = "ACF - Log-Retornos Pré queda", na.action = na.pass)
pacf(log_ret_vec[1:10000], main = "PACF - Log-Retornos Pré queda", na.action = na.pass)

par(mfrow = c(1, 2))
acf(log_ret_vec[idx_min:p2], main = "ACF - Log-Retornos Pós queda", na.action = na.pass)
pacf(log_ret_vec[idx_min:p2], main = "PACF - Log-Retornos Pós queda", na.action = na.pass)
```

## 1. Objetivos

Comparação das as funções de autocorrelação (ACF) e autocorrelação parcial (PACF) dos log-retornos intradiários de PETR4 **antes** e **depois** de uma queda da opção.

## Ajuste do Modelo de Volatilidade Estocástica

Ajustamos um modelo `svsample()` a dois blocos temporais:

1.  Primeiros 10000 pontos antes do maior retorno absoluto negativo.
2.  10000 pontos após o menor retorno (queda abrupta) para comparar regimes.
3.  15000 pontos após analise anterior 10000 mil pontos são analisados para observar a recuperação do mercado a uma queda bruta.

```{r, include=FALSE}

echo=FALSE
warning=FALSE
message=FALSE


flag_file <- "compressed.txt"

if (file.exists(flag_file)) {

  file_sv1  <- "sv_fit.rds"
  file_sv2  <- "sv_fit2.rds"
  file_sv3  <- "sv_fit3.rds"
  
  sv_fit  <- readRDS(file_sv1)
  sv_fit2 <- readRDS(file_sv2)
  sv_fit3 <- readRDS(file_sv3)
  
} else {

  reduce_and_save <- function(obj, file_out, max_draws = 2000, keep_sample_of_vector = 50, thin_method = c("systematic", "random")) {
    thin_method <- match.arg(thin_method)
    orig_class <- class(obj)
    reduction_log <- list()
    
    thin_rows <- function(mat, max_rows) {
      nr <- nrow(mat)
      if (nr > max_rows) {
        if (thin_method == "systematic") {
          idx <- unique(round(seq(1, nr, length.out = max_rows)))
        } else {
          set.seed(1)
          idx <- sort(sample(nr, size = max_rows))
        }
        return(list(mat = mat[idx, , drop = FALSE], kept = length(idx), original = nr))
      } else {
        return(list(mat = mat, kept = nr, original = nr))
      }
    }
    

    for (nm in names(obj)) {
      comp <- obj[[nm]]
      if (is.matrix(comp) || is.data.frame(comp)) {
        if (is.numeric(as.matrix(comp))) {
          res <- thin_rows(as.matrix(comp), max_draws)
          obj[[nm]] <- res$mat
          reduction_log[[nm]] <- list(type = "matrix/data.frame",
                                      original_rows = res$original,
                                      kept_rows = res$kept)
        } else {

          reduction_log[[nm]] <- list(type = "matrix_non_numeric_kept")
        }
      } else if (is.array(comp) && length(dim(comp)) >= 2 && is.numeric(comp)) {
        dims <- dim(comp)
        nfirst <- dims[1]
        if (nfirst > max_draws) {
          if (thin_method == "systematic") {
            idx <- unique(round(seq(1, nfirst, length.out = max_draws)))
          } else {
            set.seed(1)
            idx <- sort(sample(nfirst, size = max_draws))
          }
          sel <- rep(list(bquote()), length(dims))
          sel[[1]] <- idx
          new_comp <- comp[idx,,, drop = FALSE]
          obj[[nm]] <- new_comp
          reduction_log[[nm]] <- list(type = "array_thinned",
                                      dim_original = dims,
                                      dim_kept = dim(new_comp))
        } else {
          reduction_log[[nm]] <- list(type = "array_kept", dim = dims)
        }
      } else if (is.list(comp)) {
        sublog <- list()
        for (subnm in names(comp)) {
          subc <- comp[[subnm]]
          if (is.matrix(subc) || (is.array(subc) && length(dim(subc)) >= 2 && is.numeric(subc))) {
            dims <- if (is.matrix(subc)) dim(subc) else dim(subc)
            nfirst <- if (is.matrix(subc)) nrow(subc) else dims[1]
            if (nfirst > max_draws && is.numeric(subc)) {
              if (thin_method == "systematic") {
                idx <- unique(round(seq(1, nfirst, length.out = max_draws)))
              } else {
                set.seed(1)
                idx <- sort(sample(nfirst, size = max_draws))
              }
              if (is.matrix(subc)) {
                comp[[subnm]] <- subc[idx, , drop = FALSE]
              } else {
                comp[[subnm]] <- subc[idx,,, drop = FALSE]
              }
              sublog[[subnm]] <- list(action = "thinned", original_first_dim = nfirst, kept = length(idx))
            } else {
              sublog[[subnm]] <- list(action = "kept", dim = dims)
            }
          } else if (is.numeric(subc) && length(subc) > 10000) {
            set.seed(1)
            samp <- sample(length(subc), size = min(keep_sample_of_vector, length(subc)))
            comp[[subnm]] <- list(sample = subc[samp],
                                  summary = c(mean = mean(subc), sd = sd(subc), median = median(subc),
                                              q025 = quantile(subc, 0.025), q975 = quantile(subc, 0.975)))
            sublog[[subnm]] <- list(action = "vector_summarised_and_sampled", original_length = length(subc),
                                    kept_sample = length(samp))
          } else {
            sublog[[subnm]] <- list(action = "left_as_is")
          }
        }
        obj[[nm]] <- comp
        reduction_log[[nm]] <- list(type = "list", details = sublog)
      } else if (is.numeric(comp) && length(comp) > 50000) {
        set.seed(1)
        samp_idx <- sample(length(comp), size = min(keep_sample_of_vector, length(comp)))
        obj[[nm]] <- list(sample = comp[samp_idx],
                          summary = c(mean = mean(comp), sd = sd(comp), median = median(comp),
                                      q025 = quantile(comp, 0.025), q975 = quantile(comp, 0.975)))
        reduction_log[[nm]] <- list(type = "big_vector_summarised", original_length = length(comp),
                                    kept_sample = length(samp_idx))
      } else {
        reduction_log[[nm]] <- list(type = "kept_as_is")
      }
    }
    
    attr(obj, "reduced_info") <- list(
      timestamp = Sys.time(),
      max_draws = max_draws,
      thin_method = thin_method,
      keep_sample_of_vector = keep_sample_of_vector,
      reduction_log = reduction_log
    )
    class(obj) <- orig_class
    
    saveRDS(obj, file = file_out, compress = "xz")
    
    return(obj)
  }

  set.seed(123)
  ret_vec <- as.numeric(ret.1min[1:10000])
  
  file_sv1 <- "sv_fit.rds"
  if (file.exists(file_sv1)) {
    sv_fit_raw <- readRDS(file_sv1)
    sv_fit <- reduce_and_save(sv_fit_raw, file_out = file_sv1, max_draws = 2000, thin_method = "systematic")
  } else {
    sv_fit_raw <- svsample(ret_vec, draws = 5000, burnin = 1000)
    sv_fit <- reduce_and_save(sv_fit_raw, file_out = file_sv1, max_draws = 2000, thin_method = "systematic")
  }

  idx_max <- which.max(dt1$Ret.1min)
  min_idx <- idx_max - 2500
  max_idx <- idx_max + 7500
  ret_vec2 <- as.numeric(ret.1min[min_idx:max_idx])
  
  file_sv2 <- "sv_fit2.rds"
  if (file.exists(file_sv2)) {
    sv_fit2_raw <- readRDS(file_sv2)
    sv_fit2 <- reduce_and_save(sv_fit2_raw, file_out = file_sv2, max_draws = 2000, thin_method = "systematic")
  } else {
    sv_fit2_raw <- svsample(ret_vec2, draws = 5000, burnin = 1000)
    sv_fit2 <- reduce_and_save(sv_fit2_raw, file_out = file_sv2, max_draws = 2000, thin_method = "systematic")
  }
  
  min2 <- max_idx + 12000
  max2 <- min2 + 10000
  ret_vec3 <- as.numeric(ret.1min[min2:max2])
  
  file_sv3 <- "sv_fit3.rds"
  if (file.exists(file_sv3)) {
    sv_fit3_raw <- readRDS(file_sv3)
    sv_fit3 <- reduce_and_save(sv_fit3_raw, file_out = file_sv3, max_draws = 2000, thin_method = "systematic")
  } else {
    sv_fit3_raw <- svsample(ret_vec3, draws = 5000, burnin = 1000)
    sv_fit3 <- reduce_and_save(sv_fit3_raw, file_out = file_sv3, max_draws = 2000, thin_method = "systematic")
  }
  file.create(flag_file)

}

```

Analisando os dados pós e pré queda da bolsa podemos ver uma mudança principalmente nas distribuições normais de Mu, Phi e Sigma, onde podemos definir o que cada variavel nos diz como:

### Interpretação de μ (mu)

1.  **Média de longo prazo**

    -   **μ** define o valor médio ao qual Log-volatilidade reverte em longo prazo.

    -   Processos com μ maior indicam que, em média, a volatilidade tende a ficar mais elevada.

2.  **“Drift” da volatilidade latente**

    -   Atua como termo constante que “puxa” o nível de volatilidade de volta ao seu ponto de equilíbrio

    -   Ao estimar o modelo, a média pontual de μ na distribuição posterior corresponde à média aritmética.

### Interpretação de φ (phi)

1.  **Persistência (autoregressão)**

    -   **φ** atua como coeficiente AR(1) medindo a “memória” da volatilidade.

    -   Se φ≈1, choques em ht−1 têm efeito duradouro​, resultando em **clusters de volatilidade**.

    **Estacionaridade**

    -   O modelo é estacionário somente se ∣ϕ∣\<1; valores absolutos acima quebram a estabilidade do processo latente.

    -   Estimações típicas em mercados emergentes mostram φ entre 0.95 e 0.99, indicando alta persistência.

### Interpretação de σ (sigma)

1.  **Volatilidade da volatilidade**

    -   **σ** é o desvio-padrão dos choques que afetam o processo de log-volatilidade

    -   Quanto maior σ, mais pronunciadas são as flutuações de curto prazo na volatilidade.

2.  **Incerteza dinâmica**

    -   Reflete a variabilidade intrínseca na evolução da volatilidade latente, controlando a rapidez das mudanças de regimes.

    -   Modelos com σ elevado tendem a capturar melhor eventos extremos (*fat tails*) e mudanças bruscas no risco.

### O que podemos retirar das nossas observações

Em particular, **μ** é a média de longo prazo da log-volatilidade, **φ** mede a persistência ou “memória” do processo, e **σ** quantifica a volatilidade da própria volatilidade.

------------------------------------------------------------------------

## Valores Estimados

| Regime          | μ     | φ    | σ    |
|-----------------|-------|------|------|
| **Pré-queda**   | –13.8 | 0.97 | 0.16 |
| **Pós-queda**   | –13.3 | 0.99 | 0.12 |
| **Recuperação** | –14.4 | 0.98 | 0.11 |

------------------------------------------------------------------------

## Interpretação dos Parâmetros

### 1. μ — Nível Médio de Longo Prazo

-   **Pré-queda (μ ≃ –13.8):** Nível médio moderado de log-volatilidade, indicando um mercado relativamente estável antes do choque.\
-   **Pós-queda (μ ≃ –13.3):** Aumento em μ sinaliza que a volatilidade média se elevou após o choque, refletindo comportamento mais errático e risco incrementado.\
-   **Recuperação (μ ≃ –14.4):** μ abaixo do nível pré-queda sugere um período de calmaria, com volatilidade média inferior ao patamar inicial

### 2. φ — Persistência (AR(1))

-   **Pré-queda (φ ≃ 0.97):** Choques de volatilidade perduram vários minutos, caracterizando clusters de volatilidade típicos em séries financeiras.\
-   **Pós-queda (φ ≃ 0.99):** Persistência extrema, indicando que o impacto do choque permanece durante longo período e reduz a capacidade de “esquecer” choques passados.\
-   **Recuperação (φ ≃ 0.98):** Alta persistência, porém ligeiramente menor que no pós-queda, sinalizando retorno gradual a um regime menos grudado em choques passados.

### 3. σ — Volatilidade da Volatilidade

-   **Pré-queda (σ ≃ 0.16):** Flutuações bruscas no nível latente de volatilidade, revelando instabilidade moderada na variância do processo.\
-   **Pós-queda (σ ≃ 0.12):** Apesar do regime mais volátil, a dispersão das mudanças na volatilidade latente diminui, indicando choques relativamente menos extremos no pós-queda.\
-   **Recuperação (σ ≃ 0.11):** Processo de volatilidade ainda mais estável, com menor amplitude de flutuações, corroborando o retorno a um regime de baixa incerteza.

## Concluindo

A análise dos parâmetros μ, φ e σ em diferentes regimes pós-queda revela mudanças profundas na dinâmica de risco de PETR4. Antes do choque, o mercado apresentava volatilidade moderada com alta instabilidade no processo latente. Após a queda, o aumento de μ e φ combinados com menor σ pautam um regime de alta persistência e volatilidade média elevada, porém com choques menos extremos. Na fase de recuperação, todos os parâmetros retornam a patamares de menor incerteza, indicando estabilização do mercado. Essas informações são cruciais para aprimorar modelos de previsão e estratégias de hedge em opções intradiárias.

## Comparação de volatilidades

Criaremos o mesmo segmento utilizados acima como:

**Segmento A:** Valores de 1 a 10000 observações antes da queda brusca.

**Segmento B:** valores de 11600 a 22600 (2000 valores antes da queda e a queda em si).

**Segmento C:** valores de 33600 a 43600 observando a volatilidade do mercado semanas após a queda.

Iremos comparar a volatilidade realizada desses pontos para entender como era uma função pré, durante e pós uma queda brusca no mercado.

```{r}
rets <- dt1$Ret.1min
segments <- list(
  A = 1:10000,
  B = 11600:21600,
  C = 33600:43600
)
vol_list <- lapply(names(segments), function(seg_name) {
  idx <- segments[[seg_name]]
  r_seg <- rets[idx]
  vol_acum <- sqrt(cumsum(r_seg^2))
  data.frame(
    index   = idx,
    vol     = vol_acum,
    segment = seg_name
  )
})
df_vol <- bind_rows(vol_list)
ggplot(df_vol, aes(x = index, y = vol, color = segment)) +
  geom_line() +
  labs(
    title    = "Volatilidade Realizada Acumulada por Segmento de Índices",
    x        = "Índice",
    y        = "Volatilidade Realizada",
    color    = "Segmento"
  ) +
  theme_minimal()
```

```{r}
# 1) Realized Variance (RV) and Realized Volatility (sqrt(RV))
calc_RV <- function(r) {
  rv <- sum((r)^2, na.rm = TRUE)
  return(rv)
}

# 2) Bipower variation (BV)
calc_BV <- function(r) {
  # r: vector of returns (aligned in time)
  if(length(r) < 2) return(NA_real_)
  bv_raw <- sum(abs(r[-1]) * abs(r[-length(r)]), na.rm = TRUE)
  bv <- (pi/2)^(-1) * bv_raw
  return(bv)
}

# 3) Jump share
calc_jump_share <- function(r) {
  rv <- calc_RV(r)
  bv <- calc_BV(r)
  jump = max(rv - bv, 0)
  return(list(RV = rv, BV = bv, Jump = jump, JumpShare = ifelse(rv>0, jump/rv, NA)))
}

# 4) Descriptive stats
summary_stats <- function(r) {
  r <- as.numeric(na.omit(r))
  n <- length(r)
  list(
    n = n,
    mean = mean(r),
    sd = sd(r),
    median_abs_dev = mad(r),
    skewness = moments::skewness(r),
    kurtosis = moments::kurtosis(r)
  )
}

# 5) Annualization helper (you used minutos_ano = 252*390)
minutos_ano <- 252 * 390  # já no seu doc; repito para segurança
annualize_vol_from_rv <- function(rv, n_obs) {
  # rv = sum(r^2) over n_obs. We want annualized volatility.
  # var_per_obs = rv / n_obs
  # annualized_vol = sqrt(var_per_obs) * sqrt(minutos_ano)
  if(n_obs <= 0) return(NA_real_)
  vol_ann <- sqrt(rv / n_obs) * sqrt(minutos_ano)
  return(vol_ann)
}

# 6) Bootstrap CI for RV (resample returns with replacement within segment)
bootstrap_RV <- function(r, R = 2000) {
  r <- na.omit(as.numeric(r))
  n <- length(r)
  if(n < 5) return(NULL)
  bootstat <- function(data, i) {
    d <- data[i]
    sum(d^2)
  }
  b <- boot::boot(data = r, statistic = bootstat, R = R)
  ci <- boot::boot.ci(b, type = c("perc","bca"))
  list(boot = b, ci = ci)
}

# 7) Permutation test for difference in RV between two segments
perm_test_RV_diff <- function(r1, r2, R = 2000) {
  r1 <- na.omit(as.numeric(r1))
  r2 <- na.omit(as.numeric(r2))
  obs_diff <- calc_RV(r2) - calc_RV(r1)            # B - A for example
  pooled <- c(r1, r2)
  n1 <- length(r1)
  n2 <- length(r2)
  perm_diffs <- numeric(R)
  set.seed(123)
  for(i in seq_len(R)) {
    perm <- sample(pooled)
    perm_diffs[i] <- calc_RV(perm[(n1+1):(n1+n2)]) - calc_RV(perm[1:n1])
  }
  p_value <- mean(abs(perm_diffs) >= abs(obs_diff))
  list(obs_diff = obs_diff, p_value = p_value, perm_diffs = perm_diffs)
}

# --- Aplicar aos segmentos A/B/C já definidos no seu documento ----------------

segments <- list(A = 1:10000, B = 11600:21600, C = 33600:43600)
rets <- dt1$Ret.1min

calc_for_segment <- function(name, idx, rets) {
  r_seg <- as.numeric(rets[idx])
  rv <- calc_RV(r_seg)
  bv <- calc_BV(r_seg)
  jump <- max(rv - bv, 0)
  jumpshare <- ifelse(rv>0, jump / rv, NA_real_)
  n_obs <- length(na.omit(r_seg))
  vol_ann <- annualize_vol_from_rv(rv, n_obs)
  stats <- summary_stats(r_seg)
  bs <- bootstrap_RV(r_seg, R = 2000)
  list(
    name = name,
    n = n_obs,
    RV = rv,
    BV = bv,
    Jump = jump,
    JumpShare = jumpshare,
    VolAnn = vol_ann,
    mean = stats$mean,
    sd = stats$sd,
    skewness = stats$skewness,
    kurtosis = stats$kurtosis,
    boot = bs
  )
}

# run for your segments
seg_names <- names(segments)
results <- lapply(seg_names, function(sn) calc_for_segment(sn, segments[[sn]], rets))
names(results) <- seg_names

# build summary table
summary_table <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    segment = x$name,
    n = x$n,
    RV = x$RV,
    BV = x$BV,
    Jump = x$Jump,
    JumpShare = x$JumpShare,
    VolAnn = x$VolAnn,
    mean = x$mean,
    sd = x$sd,
    skewness = x$skewness,
    kurtosis = x$kurtosis
  )
}))
print(summary_table)

# --- Comparações e testes ---------------------------------------------------

# Percent differences and absolute diffs
pairwise_compare <- function(x, y) {
  abs_diff <- x - y
  pct_diff <- ifelse(y != 0, (x - y) / abs(y) * 100, NA_real_)
  list(abs_diff = abs_diff, pct_diff = pct_diff)
}

# Example: Comparing B vs A and C vs A
diff_BA <- pairwise_compare(results$B$RV, results$A$RV)
diff_CA <- pairwise_compare(results$C$RV, results$A$RV)
cat("RV differences (B - A):", diff_BA$abs_diff, "(", round(diff_BA$pct_diff,2), "% )\n")
cat("RV differences (C - A):", diff_CA$abs_diff, "(", round(diff_CA$pct_diff,2), "% )\n")

# Permutation test for RV difference B vs A
perm_BA <- perm_test_RV_diff(rets[segments$A], rets[segments$B], R = 3000)
perm_CA <- perm_test_RV_diff(rets[segments$A], rets[segments$C], R = 3000)
cat("Perm test p-value B vs A:", perm_BA$p_value, "\n")
cat("Perm test p-value C vs A:", perm_CA$p_value, "\n")

# Levene test for equality of variances of returns (A,B,C)
combined <- data.frame(
  ret = c(as.numeric(rets[segments$A]), as.numeric(rets[segments$B]), as.numeric(rets[segments$C])),
  seg = factor(c(rep("A", length(segments$A)), rep("B", length(segments$B)), rep("C", length(segments$C))))
)
levene_res <- car::leveneTest(ret ~ seg, data = combined)
print(levene_res)

# KS tests pairwise on returns distributions (A,B)
ks_AB <- ks.test(as.numeric(rets[segments$A]), as.numeric(rets[segments$B]))
ks_AC <- ks.test(as.numeric(rets[segments$A]), as.numeric(rets[segments$C]))
print(ks_AB)
print(ks_AC)

# --- Produzir um quadro final com interpretações automáticas -----------------
final_df <- summary_table %>%
  as.data.frame() %>%
  mutate(
    VolAnn_rounded = round(VolAnn, 4),
    JumpShare_pct = round(JumpShare * 100, 2)
  )
print(final_df)
```

```{r}
# minutos_ano from your doc
minutos_ano <- 252 * 390

# --- Helper: ensure we have comparable draw counts -------------------------
# A small util that concatenates chains and returns a matrix of draws.
get_para_mat <- function(svobj) {
  # para() returns an mcmc object (mcmc.list) -> as.matrix converts to a matrix draws x params
  pm <- as.matrix(para(svobj, chain = "concatenated"))
  # columns usually 'mu', 'phi', 'sigma' (and maybe others if modelled)
  return(pm)
}
get_latent_mat <- function(svobj) {
  # latent() returns latent log-vol draws (mcmc.list) -> as.matrix -> rows = draws, cols = h[1], h[2], ...
  lm <- as.matrix(latent(svobj, chain = "concatenated"))
  # each column is h_t, each row is one posterior draw
  return(lm)
}

# --- Extract draws for your three fits ------------------------------------
# (sv_fit)   -> pre (you used ret_vec <- as.numeric(ret.1min[1:10000]); sv_fit <- svsample(...)
# (sv_fit2)  -> post
# (sv_fit3)  -> recovery

para1 <- get_para_mat(sv_fit)    # matrix draws x params
para2 <- get_para_mat(sv_fit2)
para3 <- get_para_mat(sv_fit3)

latent1 <- get_latent_mat(sv_fit)   # matrix draws x time
latent2 <- get_latent_mat(sv_fit2)
latent3 <- get_latent_mat(sv_fit3)

# --- If draws counts differ, downsample to common number for pairwise comparisons
n1 <- nrow(para1); n2 <- nrow(para2); n3 <- nrow(para3)
common_draws <- min(n1, n2, n3)

set.seed(1)
sample_rows <- function(mat, n) {
  if(nrow(mat) == n) return(mat)
  idx <- sample(seq_len(nrow(mat)), n)
  mat[idx, , drop = FALSE]
}
para1s <- sample_rows(para1, common_draws)
para2s <- sample_rows(para2, common_draws)
para3s <- sample_rows(para3, common_draws)
latent1s <- sample_rows(latent1, common_draws)
latent2s <- sample_rows(latent2, common_draws)
latent3s <- sample_rows(latent3, common_draws)

# --- 1) Parameter summaries (mu, phi, sigma) -------------------------------
summ_param <- function(parmat) {
  # parmat: draws x parameters
  as.data.frame(t(apply(parmat, 2, function(x) {
    c(mean = mean(x), sd = sd(x), 
      q2.5 = quantile(x, 0.025), q97.5 = quantile(x, 0.975))
  })), row.names = NULL)
}
tab_para1 <- summ_param(para1s)
tab_para2 <- summ_param(para2s)
tab_para3 <- summ_param(para3s)

tab_para <- dplyr::bind_rows(
  cbind(regime = "pre",  param = rownames(tab_para1), tab_para1),
  cbind(regime = "post", param = rownames(tab_para2), tab_para2),
  cbind(regime = "recov", param = rownames(tab_para3), tab_para3)
)
print(tab_para)

# --- Pairwise posterior differences for parameters -------------------------
# Example: mu_post - mu_pre
param_diff <- function(matA, matB, parname) {
  # returns vector of differences B - A for draws
  diffs <- matB[, parname] - matA[, parname]
  data.frame(
    mean = mean(diffs),
    sd   = sd(diffs),
    q2.5 = quantile(diffs, 0.025),
    q97.5= quantile(diffs, 0.975),
    prob_gt0 = mean(diffs > 0)  # posterior prob that B > A
  )
}

# compute differences
diff_mu_post_pre  <- param_diff(para1s, para2s, "mu")
diff_phi_post_pre <- param_diff(para1s, para2s, "phi")
diff_sigma_post_pre <- param_diff(para1s, para2s, "sigma")

# Print
print(diff_mu_post_pre); print(diff_phi_post_pre); print(diff_sigma_post_pre)

# --- 2) Variance/Volatility from latent states ------------------------------
# instantaneous variance v_t = exp(h_t); per-draw mean variance across t:
per_draw_mean_variance <- function(latmat) {
  # latmat: draws x time (h_t)
  # compute per-draw mean variance = mean(exp(h_t)) across t
  rowMeans(exp(latmat), na.rm = TRUE)    # vector length = draws
}
per_draw_mean_sd <- function(latmat) {
  sqrt(per_draw_mean_variance(latmat))
}
per_draw_ann_vol <- function(latmat) {
  per_draw_mean_sd(latmat) * sqrt(minutos_ano)
}

v1 <- per_draw_mean_variance(latent1s)
v2 <- per_draw_mean_variance(latent2s)
v3 <- per_draw_mean_variance(latent3s)

sd1 <- sqrt(v1); sd2 <- sqrt(v2); sd3 <- sqrt(v3)
ann1 <- sd1 * sqrt(minutos_ano)
ann2 <- sd2 * sqrt(minutos_ano)
ann3 <- sd3 * sqrt(minutos_ano)

# Summaries table
summarize_draws <- function(vec) {
  c(mean = mean(vec), median = median(vec), sd = sd(vec),
    q2.5 = quantile(vec, 0.025), q97.5 = quantile(vec, 0.975))
}
tab_vol <- rbind(
  pre  = summarize_draws(ann1),
  post = summarize_draws(ann2),
  recov= summarize_draws(ann3)
)
tab_vol <- as.data.frame(tab_vol)
tab_vol$regime <- rownames(tab_vol)
tab_vol <- tab_vol[, c("regime", names(tab_vol)[1:(ncol(tab_vol)-1)])]
print(tab_vol)

# --- Pairwise comparisons of annualized vol (posterior differences) ----------
diff_ann_post_pre <- ann2 - ann1   # vector of draws
diff_ann_recov_pre <- ann3 - ann1

comp_ann <- data.frame(
  mean = c(mean(diff_ann_post_pre), mean(diff_ann_recov_pre)),
  sd   = c(sd(diff_ann_post_pre), sd(diff_ann_recov_pre)),
  q2.5 = c(quantile(diff_ann_post_pre, 0.025), quantile(diff_ann_recov_pre, 0.025)),
  q97.5= c(quantile(diff_ann_post_pre, 0.975), quantile(diff_ann_recov_pre, 0.975)),
  prob_gt0 = c(mean(diff_ann_post_pre > 0), mean(diff_ann_recov_pre > 0))
)
rownames(comp_ann) <- c("post - pre", "recov - pre")
print(comp_ann)

# --- 3) Plot posterior densities for annualized vol -------------------------
df_plot <- data.frame(
  ann = c(ann1, ann2, ann3),
  regime = factor(rep(c("pre","post","recov"),
                      times = c(length(ann1), length(ann2), length(ann3))))
)
p <- ggplot(df_plot, aes(x = ann, fill = regime)) +
  geom_density(alpha = 0.4) +
  labs(title = "Posterior densities of per-segment annualized volatility",
       x = "Annualized volatility (approx)", y = "Density") +
  theme_minimal()
print(p)

# annotate posterior probability that post > pre
prob_post_gt_pre <- mean(ann2 > ann1)
prob_recov_gt_pre <- mean(ann3 > ann1)
cat(sprintf("Posterior prob(ann_post > ann_pre) = %.3f\n", prob_post_gt_pre))
cat(sprintf("Posterior prob(ann_recov > ann_pre) = %.3f\n", prob_recov_gt_pre))

# --- 4) OPTIONAL: pointwise posterior quantiles (time-resolved) -------------
# If you want time series of posterior median vol_t and 95% band
median_vol1_t <- apply(exp(latent1s), 2, median)   # median exp(h_t) per time => variance
sd_median1_t <- sqrt(median_vol1_t)
```

```{r}
rets <- dt1$Ret.1min
segments <- list(
  A = 1:10000,
  B = 11600:22600,
  C = 33600:43600
)

vol_list2 <- lapply(names(segments), function(seg_name) {
  idx   <- segments[[seg_name]]
  vol   <- sqrt(rets[idx]^2)
  data.frame(index = idx, vol = vol, segment = seg_name)
})
df_vol_inst <- bind_rows(vol_list2)

plot_segment <- function(df, seg_name) {
  df_sub <- df %>% filter(segment == seg_name)
  ggplot(df_sub, aes(x = index, y = vol)) +
    geom_line() +
    labs(
      title = paste0("Volatilidade Realizada – Segmento ", seg_name),
      x     = "Índice",
      y     = "Volatilidade Realizada (|Retorno|)"
    ) +
    ylim(0, 0.02)+
    theme_minimal()
}

plot_A <- plot_segment(df_vol_inst, "A")
plot_B <- plot_segment(df_vol_inst, "B")
plot_C <- plot_segment(df_vol_inst, "C")

plot_A  
plot_B  
plot_C  


```

```{r}
h_mcmc <- sv_fit$latent[[1]]  
h_mat <- as.matrix(h_mcmc)     
dim(h_mat)                    
h_mean <- colMeans(h_mat)     
vol_est <- exp(h_mean / 2)   

time_index <- seq(
  from       = as.POSIXct("2021-01-01 09:31"),
  by         = "1 min",
  length.out = length(vol_est)
)


h_mcmc <- sv_fit2$latent[[1]]  
h_mat <- as.matrix(h_mcmc)     
dim(h_mat)                    
h_mean <- colMeans(h_mat)     
vol_est2 <- exp(h_mean / 2)   

time_index2 <- seq(
  from       = as.POSIXct("2021-01-01 09:31"),
  by         = "1 min",
  length.out = length(vol_est2)
)


h_mcmc <- sv_fit$latent[[1]]  
h_mat <- as.matrix(h_mcmc)     
dim(h_mat)                    
h_mean <- colMeans(h_mat)     
vol_est3 <- exp(h_mean / 2)   

time_index3 <- seq(
  from       = as.POSIXct("2021-01-01 09:31"),
  by         = "1 min",
  length.out = length(vol_est3)
)

h_mcmc <- sv_fit_complt$latent[[1]]  
h_mat <- as.matrix(h_mcmc)     
dim(h_mat)                    
h_mean <- colMeans(h_mat)     
vol_est_complt <- exp(h_mean / 2)   

time_index_complt <- seq(
  from       = as.POSIXct("2021-01-01 09:31"),
  by         = "1 min",
  length.out = length(vol_est_complt)
)

df_est <- data.frame(time = time_index, vol = vol_est)
df_est2 <- data.frame(time = time_index2, vol = vol_est2)
df_est3 <- data.frame(time = time_index3, vol = vol_est3)
```

```{r}
par(mfrow = c(3, 1))
plot(df_est)
plot(df_est2)
plot(df_est3)
```

```{r}

ggplot(df_est_complt, aes(x = time, y = vol)) +
  geom_line() +
  labs(
    title = "Volatilidade Estocástica (média posterior)",
    x     = "Tempo",
    y     = "σ̂_t"
  ) +
  theme_minimal()
```

## Decomposição MODWT

-   **Transformada**: MODWT em 12 níveis, correspondente ao máximo suportado.

-   **Onda‑mãe**: Haar, por sua simplicidade e desempenho.

-   **Interpretação**: níveis de detalhe (d1, d4, d8, d12) mostram oscilações de alta frequência ligadas ao salto de 22/02/2021; níveis de baixa frequência evidenciam a tendência geral.

```{r}
tseries::adf.test(df_est_complt$vol)
```

```{r}
df_test <- data.frame(
  time = dt1$Period,
  vol  = dt1$`Ret.1min`
)

compute_realized_vol <- function(df,
                                 return_col = "vol",
                                 time_col   = "time",
                                 window     = "5 mins") {
  df %>%
    mutate(ret2 = .data[[return_col]]^2) %>%
    mutate(time_window = floor_date(.data[[time_col]], unit = window)) %>%
    group_by(time_window) %>%
    summarise(
      realized_var = sum(ret2, na.rm = TRUE),
      vol = sqrt(realized_var),
      .groups = "drop"
    ) %>%
    rename(!!time_col := time_window)
}

df_rv5 <- compute_realized_vol(df_test,
                               return_col = "vol",
                               time_col   = "time",
                               window     = "5 mins")
```

```{r}
tseries::adf.test(df_rv5$vol)
```

```{r}
plot_wavelet_levels_modwt_hist <- function(df, levels = 12, df_name = "df_est") {
  
  filter_type <- "haar"
  max_possible_levels <- min(levels, floor(log2(nrow(df))))
  
  modwt_result <- modwt(df$vol, n.levels = max_possible_levels, boundary = "periodic")
  total_plots <- max_possible_levels + 2
  plots_list <- vector("list", total_plots)
  
  for (plot_idx in seq_len(total_plots)) {
    if (plot_idx == 1) {
      plots_list[[plot_idx]] <- ggplot(df, aes(x = time, y = vol)) +
        geom_line(color = "blue") +
        labs(title = paste(df_name, ": Série Original"), x = "Tempo", y = "Volatilidade") +
        theme_minimal()
      
    } else if (plot_idx == total_plots) {
      approx_modwt <- modwt_result
      for (j in seq_len(max_possible_levels)) approx_modwt@W[[j]][] <- 0
      approximation <- imodwt(approx_modwt)
      df_plot <- data.frame(time = df$time, value = approximation)
      
      plots_list[[plot_idx]] <- ggplot(df_plot, aes(x = time, y = value)) +
        geom_line(color = "red") +
        labs(title = paste(df_name, ": Aproximação (Nível", max_possible_levels, ")"),
             x = "Tempo", y = "Valor") +
        theme_minimal()
      
    } else {
      i <- plot_idx - 1
      detail_modwt <- modwt_result
      for (j in seq_len(max_possible_levels)) if (j != i) detail_modwt@W[[j]][] <- 0
      detail_series <- imodwt(detail_modwt)
      df_plot <- data.frame(time = df$time, value = detail_series)
      
      plots_list[[plot_idx]] <- ggplot(df_plot, aes(x = time, y = value)) +
        geom_line(color = "darkgreen") +
        labs(title = paste(df_name, ": Detalhe Nível", i), x = "Tempo", y = "Valor") +
        theme_minimal()
    }
  }

  for (k in seq(1, length(plots_list), by = 2)) {
    p1 <- plots_list[[k]]
    p2 <- if ((k + 1) <= length(plots_list)) plots_list[[k + 1]] else NULL
    if (!is.null(p2)) {
      grid.arrange(p1, p2, nrow = 2)
    } else {
      print(p1)
    }
  }
  
  invisible(plots_list)
}
#como comparar volatilidade em periodos diferentes 
plot_wavelet_levels_modwt_hist(df_est, levels = 12, df_name = "PETR4")

```

```{r}
plot_wavelet_levels_modwt_jumps <- function(df, levels = 15, df_name = "df_est", momento) {
  filter_type <- "haar"
  max_levels <- min(levels, floor(log2(nrow(df))))


  modwt_res <- modwt(df$vol, filter = filter_type, n.levels = max_levels, boundary = "reflection")
  total_plots <- max_levels + 2 
  plots <- vector("list", total_plots)

  for (idx in seq_len(total_plots)) {
    if (idx == 1) {
      p <- ggplot(df, aes(x = time, y = vol)) +
        geom_line(color = "blue") +
        labs(title = paste(df_name, ": Série Original"), x = "Tempo", y = "Volatilidade") +
        theme_minimal()

    } else if (idx == total_plots) {
      approx <- modwt_res
      for (j in seq_len(max_levels)) approx@W[[j]][] <- 0
      series_approx <- imodwt(approx)
      df_a <- data.frame(time = df$time, value = series_approx[1:nrow(df)])

      p <- ggplot(df_a, aes(x = time, y = value)) +
        geom_line(color = "red") +
        labs(title = paste(df_name, ": Aproximação (Nível", max_levels, ")"),
             x = "Tempo", y = "Valor") +
        theme_minimal()

    } else {
      i <- idx - 1
      detail <- modwt_res
      for (j in seq_len(max_levels)) if (j != i) detail@W[[j]][] <- 0
      series_det <- imodwt(detail)
      df_d <- data.frame(time = df$time, value = series_det[1:nrow(df)])
      Wj <- modwt_res@W[[i]]
      sigma_j <- median(abs(Wj), na.rm = TRUE) / 0.6745
      thr_j <- sigma_j * sqrt(2 * log(length(Wj)))
      jumps <- which(abs(Wj) > thr_j)
      jumps <- jumps[jumps <= nrow(df)]

      df_d$jumps <- NA
      df_d$jumps[jumps] <- df_d$value[jumps]

      p <- ggplot(df_d, aes(x = time, y = value)) +
        geom_line(color = "darkgreen") +
        geom_point(data = subset(df_d, !is.na(jumps)),
                   aes(x = time, y = jumps), color = "red", size = 2) +
        labs(title = paste(df_name, momento, ": Detalhe Nível", i, "com Saltos"),
             x = "Tempo", y = "Valor") +
        theme_minimal()
    }
    plots[[idx]] <- p
  }

  for (k in seq(1, length(plots), by = 2)) {
    p1 <- plots[[k]]
    p2 <- if ((k+1) <= length(plots)) plots[[k+1]] else NULL
    if (!is.null(p2)) {
      grid.arrange(p1, p2, nrow = 2)
    } else {
      print(p1)
    }
  }

  invisible(plots)
}
```

```{r}
plot_wavelet_levels_modwt_jumps(df_est, levels = 12, df_name = "PETR4", momento = "Pré queda")
```

```{r}
plot_wavelet_levels_modwt_jumps(df_est2, levels = 12, df_name = "PETR4", momento = "Durante a queda")
```

```{r}
plot_wavelet_levels_modwt_jumps(df_est3, levels = 12, df_name = "PETR4", momento = "Pós queda")
```

```{r}
detect_jumps_modwt <- function(df, levels = 12) {
  filter_type <- "haar"
  max_levels <- min(levels, floor(log2(nrow(df))))
  modwt_res <- modwt(df$vol, filter = filter_type, n.levels = max_levels, boundary = "reflection")
  jump_indices_list <- vector("list", max_levels)
  for (level in seq_len(max_levels)) {
    Wj <- modwt_res@W[[level]]
    sigma_j <- median(abs(Wj), na.rm = TRUE) / 0.6745
    thr_j <- sigma_j * sqrt(2 * log(length(Wj)))
    idx <- which(abs(Wj) > thr_j)
    jump_indices_list[[level]] <- idx[idx <= nrow(df)]
  }
  list(modwt = modwt_res, jumps = jump_indices_list)
}

model_jump_intensity <- function(df, jump_list, window = "hour", df_name = "df_est",momento) {
  intensity_plots <- list()
  for (level in seq_along(jump_list)) {
    idx <- jump_list[[level]]
    if (length(idx) == 0) next
    times <- df$time[idx]
    df_counts <- data.frame(time = times) %>%
      mutate(interval = floor_date(time, unit = window)) %>%
      count(interval)
    fit <- glm(n ~ 1, family = poisson, data = df_counts)
    lambda_hat <- exp(coef(fit))
    p <- ggplot(df_counts, aes(x = interval, y = n)) +
      geom_bar(stat = "identity", fill = "steelblue") +
      geom_hline(yintercept = lambda_hat, linetype = "dashed", color = "red") +
      labs(title = paste(df_name, momento, ": Intensidade de Saltos - Nível", level),
           subtitle = paste("Lambda estimado =", round(lambda_hat, 3)),
           x = paste("Intervalo (", window, ")", sep = ""), y = "Contagem de Saltos") +
      theme_minimal()
    intensity_plots[[level]] <- p
  }
  plot_indices <- which(!sapply(intensity_plots, is.null))
  for (k in seq(1, length(plot_indices), by = 2)) {
    p1 <- intensity_plots[[plot_indices[k]]]
    p2 <- if ((k+1) <= length(plot_indices)) intensity_plots[[plot_indices[k+1]]] else NULL
    if (!is.null(p2)) {
      grid.arrange(p1, p2, nrow = 2)
    } else {
      print(p1)
    }
  }
  invisible(intensity_plots)
}
```

```{r}
res <- detect_jumps_modwt(df_est, levels = 12)
model_jump_intensity(df_est, res$jumps, window = "hour", df_name = "PETR4",momento = "Pré queda")

```

```{r}
res2 <- detect_jumps_modwt(df_est2, levels = 12)
model_jump_intensity(df_est2, res2$jumps, window = "hour", df_name = "PETR4",momento = "Durante a queda")
```

```{r}
res3 <- detect_jumps_modwt(df_est3, levels = 12)
model_jump_intensity(df_est3, res3$jumps, window = "hour", df_name = "PETR4",momento = "Pós queda")
```

```{r load-packages, include=FALSE}
res <- detect_jumps_modwt(df_est2, levels = 12)
jumps_vec <- unlist(res$jumps)
jump_idx <- which(!is.na(jumps_vec) & jumps_vec != 0)
cat("Saltos detectados em", length(jump_idx), "instantes\n")
ret       <- df_est2$vol
ret_clean <- ret
ret_clean[jump_idx] <- 0

sv_fit_orig <- svsample(y = ret,       draws = 5000, burnin = 1000,
                        priormu = c(0,10), priorphi = c(20,1.1),
                        priorsigma = 1)
sv_fit_clean<- svsample(y = ret_clean, draws = 5000, burnin = 1000,
                        priormu = c(0,10), priorphi = c(20,1.1),
                        priorsigma = 1)
```

```{r}
h_draws_orig  <- latent(sv_fit_orig)
h_draws_clean <- latent(sv_fit_clean)
h_mean_orig  <- colMeans(as.matrix(h_draws_orig))
h_mean_clean <- colMeans(as.matrix(h_draws_clean))
h_mean_orig  <- h_mean_orig[-1]
h_mean_clean <- h_mean_clean[-1]
sigma_orig  <- exp(h_mean_orig / 2)
sigma_clean <- exp(h_mean_clean / 2)
library(ggplot2)
p1 <- ggplot(data = data.frame(time = df_est$time, sigma = sigma_orig),
             aes(x = time, y = sigma)) +
  geom_line() +
  ggtitle("SV: Série Original (com saltos)") +
  xlab("Tempo") + ylab("σₜ")

p2 <- ggplot(data = data.frame(time = df_est$time, sigma = sigma_clean),
             aes(x = time, y = sigma)) +
  geom_line() +
  ggtitle("SV: Série Limpa (saltos zerados)") +
  xlab("Tempo") + ylab("σₜ")

library(gridExtra)
grid.arrange(p1, p2, nrow = 2)

spec_garch <- ugarchspec(
  variance.model     = list(model = "sGARCH", garchOrder = c(1, 1)),
  mean.model         = list(armaOrder = c(0, 0), include.mean = FALSE),
  distribution.model = "std"
)

fit_garch_orig  <- ugarchfit(spec = spec_garch, data = ret,       solver = "hybrid")
fit_garch_clean <- ugarchfit(spec = spec_garch, data = ret_clean, solver = "hybrid")

aic_orig   <- infocriteria(fit_garch_orig)["Akaike"]
bic_orig   <- infocriteria(fit_garch_orig)["Bayes"]
aic_clean  <- infocriteria(fit_garch_clean)["Akaike"]
bic_clean  <- infocriteria(fit_garch_clean)["Bayes"]

resid_orig <- residuals(fit_garch_orig, standardize = TRUE)
acf(resid_orig, main = "ACF Resíduos Padronizados (Orig.)")
pacf(resid_orig, main = "PACF Resíduos Padronizados (Orig.)")
```

## Descrição dos Dados

Para ilustrar a identificação de saltos em diferentes escalas, foi utilizada a série de preços da PETR4 (Petrobrás) no período de 04/01/2021 a 25/06/2021, com frequência de 1 minuto. A escolha deste ativo deve-se à sua alta liquidez e influência sobre o Ibovespa, além de um evento de queda acentuada em 22/02/2021, motivado pelo anúncio de troca de presidência da empresa.

-   **Fonte**: MetaTrader 5.

-   **Período**: 04/01/2021 a 25/06/2021 (119 dias, 49 611 observações).

-   **Ajustes**: remoção dos primeiros 19 minutos após abertura (10h20 em diante), para evitar ruídos de leilão de abertura.

## Próximos Passos

-   Como reproduzir a serie estocastica.
-   Identificar saltos na serie estocastica limite universal.
-   tentar deconpor em ondaleta se possivel a serie estocastica.

ler barunik, arruma ondeleta, saltos intradiarios, definir vetor na serie completa (onde não tiver saltos = 0, onde tiver salto - valor do indice anterior) calculo no Jv
